{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer for human number sequence prediction\n",
        "\n",
        "[Fake college football](https://www.reddit.com/r/FakeCollegeFootball/) is a game where two teams, an offense and a defense, select numbers 1-1500. If those numbers are close together, the offense has a good play and if those numbers are far apart, the defense has a good play (note that 1 and 1500 are 1 apart, as the numbers are in a circle). The game takes around 100 numbers to complete.\n",
        "\n",
        "Given the sequential nature of the game, I constructed a transformer to predict the probability that the next number submitted by a player is within each of the 15 100 number groups (1-100, 101-200, etc.)."
      ],
      "metadata": {
        "id": "oY1wunkmdJ0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TwFiOktPAGU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "usernames = [\"nusm\",\"belikemefr\",\"mycatsananalasshole\",\"therealslimcampbell\",\"ihasmagyk\",\"prankishpoet\",\"bearded_wildcard\",\"mile114\",\"vrain_19\",\n",
        "    \"crabface5\",\"wazzup44\",\"callmefoofking\",\"erduck96\",\"bighoppy75\",\"metsareawesome5\",\"yeeting_man\",\"egglton\",\"mississippimadness\",\"rph2003\",\n",
        "    \"snasty728\",\"steelersforlife11\",\"skeleton-with-skin10\",\"tehmoofish\",\"l3ach13\",\"6g0d757\",\"rickypop\",\"davy_grolton\",\"broo_lynn\",\"klarge_\",\n",
        "    \"alternateshapes\",\"spanxc\",\"3rantgp\",\"creatxrcreator\",\"dj_bradlezzz\",\"naragog1\",\"minimum_junket9199\",\"psujosh\",\"baetaro\",\"sonerandomuser\",\n",
        "    \"birdmanthethird\",\"jakester1238\",\"americansasquatch_24\",\"qc_undercover\",\"despacitoritzcracker\",\"kelbo11\",\"the_hoff34\",\"torchedpineapple\",\n",
        "    \"rocco5w\",\"horribelspelling\",\"hobbes_t_hero\",\"scum-phoenix\",\"scsprinter13\",\"astockusername\",\"callofmc\",\"_toughscene\",\"dark197\",\"natestate\",\n",
        "    \"jsteele1423\",\"hman1500\",\"sexy-chicagoan-1837\",\"thebattlersprince\",\"bearinthewoods1\",\"matt__17\",\"ethed99\",\"edgerocks2\",\"door_nav\",\"leruul\",\n",
        "    \"tee_jay9\",\"zachfischer2528\",\"shoonipatooti\",\"inatro\",\"oogadebob\",\"fyre87\"]\n",
        "# Assuming the list of dictionaries is stored in a variable called 'data'\n",
        "plays_data = []\n",
        "for username in usernames:\n",
        "  # API endpoint URL\n",
        "  url = \"https://api.1212.one/plays/coach/\" + username\n",
        "  try:\n",
        "    response = requests.get(url, timeout=10)\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(\"Error:\", e)\n",
        "    print(f\"Failed on username: {username}\")\n",
        "    continue\n",
        "\n",
        "\n",
        "  # Make a GET request to the API\n",
        "  if response.status_code == 200:\n",
        "      # Request was successful\n",
        "      data = response.json()  # Parse the response as JSON\n",
        "      print(f\"Successfully fetched data for username: {username}\")\n",
        "  else:\n",
        "      # Request failed\n",
        "      print(f\"Request failed with status code: {response.status_code}\")\n",
        "      print(f\"Failed on username: {username}\")\n",
        "      continue\n",
        "\n",
        "\n",
        "  playerNumber = -1\n",
        "  opponentNumber = -1\n",
        "\n",
        "  for play in data:\n",
        "      # If too early, then not all data available including play number. Ignore those games\n",
        "      if not \"playNumber\" in play.keys():\n",
        "        continue\n",
        "      if play['playNumber'] == 1:\n",
        "        playerNumber = -1\n",
        "        opponentNumber = -1\n",
        "\n",
        "      if opponentNumber != None and playerNumber != None:\n",
        "        play_data = {\n",
        "            'gameId': play['game']['gameId'],\n",
        "            'playNumber': play['playNumber'],\n",
        "            'playerNumber': float(playerNumber),\n",
        "            'opponentNumber': float(opponentNumber),\n",
        "            'down': play['down'],\n",
        "            'distance': play['distance'],\n",
        "            'quarter': play['quarter'],\n",
        "            'clock': play['clock'],\n",
        "            # 'yards': play['yards'],\n",
        "            # 'yardLine': play['yardLine'],\n",
        "            'onOffense': int(play['coachIsOffense'])\n",
        "        }\n",
        "        # Used to shift numbers down one row. Because each row then holds previous numbers played and what down / offense it is\n",
        "        offense = int(play['coachIsOffense'])\n",
        "        if offense == 1:\n",
        "          # Player is on offense\n",
        "          playerNumber = play['offense']['number']\n",
        "          opponentNumber = play['defense']['number']\n",
        "        else:\n",
        "          # Player is on defense\n",
        "          playerNumber = play['defense']['number']\n",
        "          opponentNumber = play['offense']['number']\n",
        "        plays_data.append(play_data)\n",
        "\n",
        "df = pd.DataFrame(plays_data)\n",
        "\n",
        "# Remove overtime stuff\n",
        "df = df[~(df['quarter']>=5)]\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSyionAGi7PS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the bins and labels for the categories\n",
        "bins = [-np.inf, 1, 51, 101, 151, 201, 251, 301, 351, 401, 451, 501, 551, 601, 651, 701, 751, 801, 851, 901, 951, 1001, 1051, 1101, 1151, 1201, 1251, 1301, 1351, 1401, 1451, np.inf]\n",
        "labels = [-1] + list(range(0, 30))\n",
        "\n",
        "# Chunk the \"playerNumber\" column into categories\n",
        "df['playerNumber']   = pd.cut(df['playerNumber'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Chunk the \"opponentNumber\" column into categories\n",
        "df['opponentNumber'] = pd.cut(df['opponentNumber'], bins=bins, labels=labels, right=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPyuLwyOJcnV"
      },
      "outputs": [],
      "source": [
        "# Create a new column to indicate the start of each game\n",
        "df['new_game'] = df['gameId'] != df['gameId'].shift(1)\n",
        "\n",
        "# Initialize an empty list to store the sequences\n",
        "sequences = []\n",
        "\n",
        "# Iterate over the rows of the DataFrame\n",
        "for _, row in df.iterrows():\n",
        "    # Tokenize the row (replace this with your actual tokenization logic)\n",
        "    tokens = [row[col] for col in df.columns[1:-1]]\n",
        "\n",
        "    sequences.append(tokens)\n",
        "\n",
        "# Split the sequences into chunks of length 8\n",
        "# input_sequences = [sequences[i:i+8] for i in range(0, len(sequences), 8)]\n",
        "data = torch.tensor(sequences)\n",
        "data[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FFIfxBgF4IJ"
      },
      "source": [
        "# Now scale and reorder data\n",
        "### Order is now:\n",
        "1. Play (numeric)\n",
        "2. Yards to Go (numeric)\n",
        "3. Clock (numeric)\n",
        "4. Your number (31 categories)\n",
        "5. Opponent number (31 categories)\n",
        "6. Down (4 categories)\n",
        "7. Quarter (4 categories)\n",
        "8. Offense (2 categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV3fwefL2FUU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Now one hot encode categorical stuff and scale numeric stuff\n",
        "\n",
        "numeric_columns = [0, 4, 6]\n",
        "categorical_columns = [1, 2, 3, 5, 7]\n",
        "\n",
        "categories = [[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
        "              [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
        "              [1, 2, 3, 4],\n",
        "              [1, 2, 3, 4],\n",
        "              [0, 1]]\n",
        "\n",
        "# Convert to numpy and encode\n",
        "encoder = OneHotEncoder(categories=categories, sparse_output=False)\n",
        "one_hot_encoded = encoder.fit_transform(data[:, categorical_columns])\n",
        "\n",
        "# Append it back\n",
        "remaining_data = data[:, numeric_columns]\n",
        "data = np.hstack((remaining_data, one_hot_encoded))\n",
        "\n",
        "# Convert back to a PyTorch tensor if needed\n",
        "data = torch.tensor(data, dtype=torch.float)\n",
        "\n",
        "\n",
        "# Scale the vars now:::\n",
        "# data[:, [0, 1, 2]] = torch.where(data[:, [0, 1, 2]] == -1, torch.tensor(0), data[:, [0, 1, 2]])\n",
        "data[:, 0] = data[:, 0]/200\n",
        "data[:, 1] = data[:, 1]/10\n",
        "data[:, 2] = data[:, 2]/420\n",
        "\n",
        "\n",
        "\n",
        "# Remove the -1 category columns\n",
        "columns_to_remove = [3, 34]\n",
        "mask = [i for i in range(data.shape[1]) if i not in columns_to_remove]\n",
        "data = data[:, mask]\n",
        "data[6:8]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer code\n",
        "\n",
        "Transformer custom built for number sequence prediction"
      ],
      "metadata": {
        "id": "VbDhX-quR5ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from utils import DEVICE\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of the self-attention layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, num_embed, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(num_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(num_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(num_embed, head_size, bias=False)\n",
        "        # tril is a lower triangular matrix. it is not a parameter\n",
        "        # of the model, so we assign it to the module using register_buffer\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # let's also add dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape # (Batch Size, Context Length, Embedding Size)\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        # compute attention scores\n",
        "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        # Tril matrix (lower triagular matrix) is used to mask\n",
        "        # future positions (setting them to -inf) so that the\n",
        "        # decoder \"learns\" to predict next words\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        # weighted aggregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v  # (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple Heads of self-attention in parallel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, num_embed, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [\n",
        "                AttentionHead(\n",
        "                    head_size=head_size,\n",
        "                    num_embed=num_embed,\n",
        "                    block_size=block_size,\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "                for _ in range(num_heads)\n",
        "            ]\n",
        "        )\n",
        "        self.proj = nn.Linear(num_embed, num_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # output of the self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # apply the linear projection layer\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by ReLu\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embed, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # in the Attention is All You Need paper\n",
        "            # authors are using the size of the ffwd layer 2048\n",
        "            # and the output of the model is 512\n",
        "            # so we apply the same factor of 4\n",
        "            nn.Linear(num_embed, 4 * num_embed),\n",
        "            nn.ReLU(),\n",
        "            # apply the linear projection layer\n",
        "            nn.Linear(4 * num_embed, num_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This calss will group together MultiHead Attention and\n",
        "    FeedForward NN, so that we can copy it in Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, block_size, num_embed, dropout):\n",
        "        super().__init__()\n",
        "        head_size = num_embed // num_heads\n",
        "        self.sa = MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            head_size=head_size,\n",
        "            num_embed=num_embed,\n",
        "            block_size=block_size,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.ffwd = FeedForward(num_embed=num_embed, dropout=dropout)\n",
        "        # add the layer normalization\n",
        "        self.ln1 = nn.LayerNorm(num_embed)\n",
        "        self.ln2 = nn.LayerNorm(num_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # \"x +\" is the skip (or residual) connection\n",
        "        # it helps with optimization\n",
        "        # also we apply layer normalization before self-attention\n",
        "        # and feed-forward (a reshufle from original paper)\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        # a simple lookup table that stores embeddings of a fixed dictionary and size\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        # see more: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "        self.input_size = kwargs.get(\"input_size\", 75)\n",
        "        self.output_size = kwargs.get(\"output_size\", 30)\n",
        "        self.num_embed = kwargs.get(\"num_embed\", 32)\n",
        "        self.block_size = kwargs.get(\"block_size\", 8)\n",
        "        self.num_heads = kwargs.get(\"num_heads\", 4)\n",
        "        self.num_layers = kwargs.get(\"num_layers\", 4)\n",
        "        self.dropout = kwargs.get(\"dropout\", 0.2)\n",
        "        # each token reads the logits for the next token from a lookup table\n",
        "        # self.token_embedding_table = nn.Embedding(self.vocab_size, self.num_embed)\n",
        "        # each position from 0 to block_size-1 will get its embedding\n",
        "        self.embedding_creator = nn.Linear(self.input_size, self.num_embed)\n",
        "        # self.embedding_creator = torch.rand(self.input_size, self.num_embed, device=DEVICE)\n",
        "        self.position_embedding_table = nn.Embedding(self.block_size, self.num_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                TransformerBlock(\n",
        "                    num_heads=self.num_heads,\n",
        "                    block_size=self.block_size,\n",
        "                    num_embed=self.num_embed,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                for _ in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        # we add the layer norm before the Linear layer\n",
        "        self.ln_f = nn.LayerNorm(self.num_embed)\n",
        "        self.lm_head = nn.Linear(self.num_embed, self.output_size) # output logits for each output category\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        #**\n",
        "        B, T, _ = idx.shape\n",
        "\n",
        "        # idx and targets are (B,T) tensor of integers\n",
        "        # the token_emb is (B, T, C), C = NUM_EMBED\n",
        "        # token_emb = self.token_embedding_table(idx)\n",
        "        token_emb = self.embedding_creator(idx)\n",
        "        # token_emb = torch.matmul(idx, self.embedding_creator)\n",
        "\n",
        "        # (T, C)\n",
        "        posit_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
        "\n",
        "        x = token_emb + posit_emb\n",
        "        # apply one head of self-attention\n",
        "        x = self.blocks(x)\n",
        "        # (B, T, vocab_size)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Get the targets, which is the players next number\n",
        "        targets = torch.argmax(targets[:, :, 3:3+self.output_size], axis=2)\n",
        "\n",
        "        # compute the loss\n",
        "        if targets != None:\n",
        "            # cross_entropy accepts inputs in a (batch_size, num_classes)\n",
        "            # so we need to reformat our logits dimensions to\n",
        "            # (batch_size * time, dim_vocabulary), time = block_size\n",
        "\n",
        "\n",
        "            # Change this so only works with min context of 8. Don't care about 0 shot predictions\n",
        "            logits = logits[:, MIN_CONTEXT:, :]\n",
        "            targets = targets[:, MIN_CONTEXT:]\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "\n",
        "            logits = torch.reshape(logits, (B * T, C))\n",
        "            targets = torch.reshape(targets, (B * T,))\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int, block_size: int):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop the context too the  last block_size tokens\n",
        "            # because tokens don't communicate between blocks\n",
        "            idx_crop = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(idx_crop)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution with probabilities probs\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def encode(text_seq: str, tokenizer: any) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to encode input text using a pre-trained tokenizer and vectorized lookups\n",
        "    \"\"\"\n",
        "    # tokenize the input text\n",
        "    tokens = tokenizer.tokenize(text_seq)\n",
        "    # convert the tokens to their corresponding ids\n",
        "    token_indices = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    token_indices = torch.tensor(token_indices, dtype=torch.long)\n",
        "    return token_indices\n",
        "\n",
        "\n",
        "def decode(enc_sec: torch.Tensor, tokenizer: any) -> str:\n",
        "    \"\"\"\n",
        "    Function to decode a sequence of token indices back to a string\n",
        "    \"\"\"\n",
        "    # convert the indices to a list\n",
        "    enc_sec = enc_sec.tolist()\n",
        "    # decode the indices to a string\n",
        "    text = tokenizer.decode(enc_sec)\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_batch(data: list[str], block_size: int, batch_size: int, valid_indices):\n",
        "    \"\"\"\n",
        "    This is a simple function to create batches of data.\n",
        "    GPUs allow for parallel processing we can feed multiple chunks at once\n",
        "    so that's why we would need batches - how many independant sequences\n",
        "    will we process in parallel.\n",
        "\n",
        "    Parameters:\n",
        "    data: list[str]: data to take batch from\n",
        "    block_size (int): size of the text that is proccessed at once (AKA CONTEXT LENGTH - WILL)\n",
        "    batch_size (int): number of sequences to process in parallel\n",
        "\n",
        "    Returns:\n",
        "    x, y: a tuple with token sequence and token target\n",
        "    \"\"\"\n",
        "    ix = valid_indices[torch.randint(len(valid_indices), (batch_size,))] # New ix based on valid indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # we stack batch_size rows of sentences\n",
        "    # so x and y are the matrices with rows_num=batch_size\n",
        "    # and col_num=block_size\n",
        "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    # y is x shifted one position right - because we predict\n",
        "    # word in y having all the previous words as context\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(\n",
        "    data: list[str],\n",
        "    model: torch.nn.Module,\n",
        "    block_size: int,\n",
        "    batch_size: int,\n",
        "    eval_iters: int = 10,\n",
        "    valid_indices=None,\n",
        "):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(data=data, block_size=block_size, batch_size=batch_size, valid_indices=valid_indices)\n",
        "        logits, loss = model.forward(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_predictions(\n",
        "    data: list[str],\n",
        "    model: torch.nn.Module,\n",
        "    block_size: int,\n",
        "    batch_size: int,\n",
        "    eval_iters: int = 10,\n",
        "    valid_indices=None,\n",
        "):\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "\n",
        "    X, Y = get_batch(data=data, block_size=block_size, batch_size=batch_size, valid_indices=valid_indices)\n",
        "    logits, loss = model.forward(X, Y)\n",
        "\n",
        "    model.train()\n",
        "    return logits\n",
        "\n",
        "\n",
        "def load_model_from_checkpoint(\n",
        "    model_class: torch.nn.Module,\n",
        "    path_to_checkpoint: str = \"checkpoints/state_dict_model.pt\",\n",
        "    **kwargs: dict,\n",
        ") -> torch.nn.Module:\n",
        "    try:\n",
        "        state_dict = torch.load(path_to_checkpoint)\n",
        "        print(\"Successfully loaded model from the checkpoint\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading the model from the checkpoint. {e}\")\n",
        "\n",
        "    model = model_class(**kwargs)\n",
        "    # load the state_dict into the model\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_model_to_chekpoint(\n",
        "    model: torch.nn.Module, path_to_checkpoint: str = \"checkpoints\", epoch: int = 0\n",
        "):\n",
        "    # check if path exists, otherwise create it\n",
        "    if not os.path.exists(path_to_checkpoint):\n",
        "        os.makedirs(path_to_checkpoint)\n",
        "\n",
        "    # datetime object containing current date and time\n",
        "    now = datetime.now()\n",
        "    # dd/mm/YY H:M:S\n",
        "    dt_string = now.strftime(\"%d.%m.%Y_%H:%M:%S\")\n",
        "    checkpoint_name = \"checkpoint_epoch-\" + str(epoch) + \"_\" + dt_string + \".pt\"\n",
        "    full_path = os.path.join(path_to_checkpoint, checkpoint_name)\n",
        "    try:\n",
        "        torch.save(model.state_dict(), full_path)\n",
        "        print(\"Successfully saved the model to {}\".format(full_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving the model to checkpoint. {e}\")"
      ],
      "metadata": {
        "id": "N937HHEPNGDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom loss function\n",
        "\n",
        "Calculates the distance between the two numbers (where 1 and 1500 are 1 apart).\n",
        "\n",
        "This method is slow and not clearly better than cross entropy."
      ],
      "metadata": {
        "id": "65Jb7H_7f5nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This method is very slow\n",
        "def reddit_football_loss(logits, target_index):\n",
        "    \"\"\"\n",
        "    Custom loss function for logits and a single target index.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): Tensor of shape (batch_size, num_classes) with logits.\n",
        "        target_index (int): The correct index (ground truth).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The computed custom loss.\n",
        "    \"\"\"\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get batch size and number of classes\n",
        "    batch_size, num_classes = probabilities.shape\n",
        "\n",
        "    # Create a tensor to hold the loss for each element in the batch\n",
        "    loss = torch.zeros(batch_size, device=DEVICE)\n",
        "\n",
        "    # Calculate the custom loss for each example in the batch\n",
        "    for i in range(batch_size):\n",
        "        target = target_index[i].item()\n",
        "        for offset in range(1, (num_classes // 2) + 1):\n",
        "            if offset == (num_classes // 2):\n",
        "              # On last time, don't double count\n",
        "              # NOTE:: #** could be good to double count cuz being exactly wrong is real bad!\n",
        "              left_index = (target - offset) % num_classes\n",
        "              loss[i] += offset * (probabilities[i, left_index])\n",
        "            else:\n",
        "              left_index = (target - offset) % num_classes\n",
        "              right_index = (target + offset) % num_classes\n",
        "              loss[i] += offset * (probabilities[i, left_index] + probabilities[i, right_index])\n",
        "\n",
        "    # Return the mean loss over the batch\n",
        "    return loss.mean()\n",
        "\n",
        "# Example usage\n",
        "# logits = torch.randn(3, 30)  # Example logits for a batch of 3 samples and 30 classes\n",
        "logits = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
        "target_index = torch.tensor([1, 0, 0])  # Example target indices for each sample in the batch\n",
        "\n",
        "loss = reddit_football_loss(logits, target_index)\n",
        "print(\"Random loss: \", loss.item())"
      ],
      "metadata": {
        "id": "T2v6i5K12qqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "RpdiSYAOR_GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "# from model import Transformer\n",
        "from transformers import AutoTokenizer  # pip install transformers\n",
        "BATCH_SIZE = 32  # how many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 12  # what is the maximum context length for predictions?\n",
        "MIN_CONTEXT = 6  # Minimum context length for training, aka only do backprop on guesses where you had at least MIN_CONTEXT other things in context window\n",
        "MAX_ITER = 10000  # number of training iterations\n",
        "EVAL_INTER = 100\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_HEAD = 4\n",
        "NUM_EMBED = NUM_HEAD * 128\n",
        "NUM_LAYER = 4\n",
        "INPUT_SIZE = data.shape[1]\n",
        "OUTPUT_SIZE = 30 # Number of categories\n",
        "DROPOUT = 0\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# load model from checkpoint\n",
        "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
        "\n",
        "# example to decode sequence\n",
        "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
        "# max_new_tokens=20)[0].tolist()\n",
        "# print(decode(vocab=vocab, enc_sec=enc_sec))\n",
        "\n",
        "\n",
        "# Train and val data\n",
        "train_data = data[0:int(0.7*data.shape[0])]\n",
        "val_data = data[int(0.7*data.shape[0]):]\n",
        "\n",
        "\n",
        "# Get valid indices to use in the training loop (Cant use indices where you will flip into the next game)\n",
        "valid_indices_train = []\n",
        "for i in range(BLOCK_SIZE, train_data.shape[0]):\n",
        "  if train_data[i, 0] > train_data[i-BLOCK_SIZE, 0]:\n",
        "    # If the current thing is still higher than all the old ones we good!\n",
        "    valid_indices_train.append(i-BLOCK_SIZE) # Start at 8, append 0 if its valid 8 out\n",
        "  else:\n",
        "    i += BLOCK_SIZE+1\n",
        "  # if data[i, 0] == data[i-BLOCK_SIZE, 0]+BLOCK_SIZE/200:\n",
        "  #   valid_indices.append(i-BLOCK_SIZE) # Start at 8, append 0 if its valid 8 out\n",
        "valid_indices_train = torch.tensor(valid_indices_train)\n",
        "\n",
        "\n",
        "valid_indices_val = []\n",
        "for i in range(BLOCK_SIZE, val_data.shape[0]):\n",
        "  if val_data[i, 0] > val_data[i-BLOCK_SIZE, 0]:\n",
        "    # If the current thing is still higher than all the old ones we good!\n",
        "    valid_indices_val.append(i-BLOCK_SIZE) # Start at 8, append 0 if its valid 8 out\n",
        "  else:\n",
        "    i += BLOCK_SIZE+1\n",
        "valid_indices_val = torch.tensor(valid_indices_val)\n",
        "\n",
        "\n",
        "\n",
        "# train a new model\n",
        "model = Transformer(\n",
        "    # vocab_size=vocab_size,\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    input_size=INPUT_SIZE,\n",
        "    num_embed=NUM_EMBED,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    num_heads=NUM_HEAD,\n",
        "    num_layers=NUM_LAYER,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "# load model to GPU if available\n",
        "m = model.to(DEVICE)\n",
        "# print the number of parameters in the model\n",
        "print(\n",
        "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
        ")\n",
        "# optimizer takes the model's parameters and the learning rate as input,\n",
        "# and updates the parameters during the training process in order to\n",
        "# minimize the loss function.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for step in tqdm(range(MAX_ITER)):\n",
        "\n",
        "    # every EVAL_INTER evaluate the loss on train and val sets\n",
        "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
        "        loss_train = estimate_loss(\n",
        "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE, valid_indices=valid_indices_train, eval_iters=50\n",
        "        )\n",
        "        train_loss.append(loss_train)\n",
        "\n",
        "        loss_val = estimate_loss(\n",
        "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE, valid_indices=valid_indices_val, eval_iters=50\n",
        "        )\n",
        "        val_loss.append(loss_val)\n",
        "\n",
        "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE, valid_indices=valid_indices_train)\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # backward() method on the loss variable calculates the gradients\n",
        "    # of the loss with respect to the model's parameters.\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # step() method on the optimizer updates the model's parameters\n",
        "    # using the calculated gradients, in order to minimize the loss.\n",
        "    optimizer.step()\n",
        "\n",
        "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoints\", epoch=step)\n"
      ],
      "metadata": {
        "id": "anEvqzpOR7eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss[1:], label='Training Loss', marker='o')\n",
        "plt.plot(val_loss[1:], label='Validation Loss', marker='x')\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MDxWW3JCNRar"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}